confint(model_reduced_collinearity_EUM , level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_EUM )
model_reduced_collinearity_EUM <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + extra_urban_metric + noise_level, data = tr_s_high_leverage)
summary(model_reduced_collinearity_EUM )
confint(model_reduced_collinearity_EUM , level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_EUM)
x <-regsubsets(co2_emission~year + euro_standard + transmission_type + engine_capacity +
fuel_type + combined_metric  + noise_level, data=tr_s_high_leverage, nvmax = 7, method = "seqrep")
summary(x)
x <-regsubsets(co2_emission~euro_standard + transmission_type + engine_capacity +
fuel_type + combined_metric  + noise_level, data=tr_s_high_leverage, nvmax = 7, method = "seqrep")
summary(x)
x <-regsubsets(co2_emission~euro_standard + transmission_type + engine_capacity +
fuel_type + combined_metric  + noise_level, data=tr_s_high_leverage, nvmax = 6, method = "seqrep")
summary(x)
set.seed(100)
model_reduced_collinearity_CM <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric + engine_capacity, data = tr_s_high_leverage)
summary(model_reduced_collinearity_CM)
confint(model_reduced_collinearity_CM, level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_CM)
set.seed(100)
model_reduced_collinearity_CM <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric, data = tr_s_high_leverage)
summary(model_reduced_collinearity_CM)
confint(model_reduced_collinearity_CM, level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_CM)
anova(x, model_reduced_collinearity_CM)
x <-regsubsets(co2_emission~euro_standard + transmission_type + engine_capacity +
fuel_type + combined_metric  + noise_level, data=tr_s_high_leverage, nvmax = 6, method = "seqrep")
summary(x)
set.seed(100)
model_reduced_collinearity_CM <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric + engine_capacity, data = tr_s_high_leverage)
summary(model_reduced_collinearity_CM)
confint(model_reduced_collinearity_CM, level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_CM)
anova(x, model_reduced_collinearity_CM)
x_anova <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric + engine_capacity + noise_level, data = tr_s_high_leverage)
x <-regsubsets(co2_emission~euro_standard + transmission_type + engine_capacity +
fuel_type + combined_metric  + noise_level, data=tr_s_high_leverage, nvmax = 6, method = "seqrep")
summary(x)
set.seed(100)
model_reduced_collinearity_CM <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric + engine_capacity, data = tr_s_high_leverage)
summary(model_reduced_collinearity_CM)
confint(model_reduced_collinearity_CM, level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_CM)
set.seed(100)
model_reduced_collinearity_CM_1 <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric, data = tr_s_high_leverage)
summary(model_reduced_collinearity_CM)
confint(model_reduced_collinearity_CM, level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_CM)
anova(model_reduced_collinearity_CM_1, model_reduced_collinearity_CM)
set.seed(100)
model_reduced_collinearity_CM_1 <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric, data = tr_s_high_leverage)
summary(model_reduced_collinearity_CM)
confint(model_reduced_collinearity_CM, level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_CM)
set.seed(100)
model_reduced_collinearity_CM_1 <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric, data = tr_s_high_leverage)
summary(model_reduced_collinearity_CM_1)
confint(model_reduced_collinearity_CM_1, level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_CM)
anova(model_reduced_collinearity_CM_1, model_reduced_collinearity_CM)
set.seed(100)
x <-regsubsets(co2_emission~yco2_emission ~ euro_standard + transmission_type +
fuel_type + fuel_cost_6000_miles   + noise_level + engine_capacity, data=tr_s_high_leverage, nvmax = 7, method = "seqrep")
summary(x)
model_reduced_collinearity_FC6000 <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + fuel_cost_6000_miles + engine_capacity, data = tr_s_high_leverage)
summary(model_reduced_collinearity_FC6000 )
confint(model_reduced_collinearity_FC6000 , level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_FC6000 )
set.seed(100)
model_reduced_collinearity_FC6000 <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + fuel_cost_6000_miles, data = tr_s_high_leverage)
summary(model_reduced_collinearity_FC6000 )
confint(model_reduced_collinearity_FC6000 , level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_FC6000 )
model_reduced_collinearity_FC6000 <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + fuel_cost_6000_miles + engine_capacity, data = tr_s_high_leverage)
summary(model_reduced_collinearity_FC6000 )
confint(model_reduced_collinearity_FC6000 , level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_FC6000 )
set.seed(100)
model_reduced_collinearity_FC6000 <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + fuel_cost_6000_miles, data = tr_s_high_leverage)
summary(model_reduced_collinearity_FC6000 )
confint(model_reduced_collinearity_FC6000 , level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_FC6000 )
set.seed(100)
model_reduced_collinearity_CM <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric + engine_capacity, data = tr_s_high_leverage)
summary(model_reduced_collinearity_CM)
confint(model_reduced_collinearity_CM, level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_CM)
# Si Ã¨ proceduto ad eliminare noise_level visto dal bestSubset Selection per poi procedere all'eliminazione
# di engine_capacity visto dall'intervallo di confidenza e year visto sia dall'intervallo di confidenza che dal p-value
set.seed(100)
model_reduced_collinearity_CM_1 <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric, data = tr_s_high_leverage)
summary(model_reduced_collinearity_CM_1)
confint(model_reduced_collinearity_CM_1, level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_CM)
set.seed(100)
x <-regsubsets(co2_emission~yco2_emission ~ euro_standard + transmission_type +
fuel_type + fuel_cost_6000_miles   + noise_level + engine_capacity, data=tr_s_high_leverage, nvmax = 7, method = "seqrep")
summary(x)
model_reduced_collinearity_FC6000 <- lm(co2_emission ~ euro_standard + transmission_type +
fuel_type + fuel_cost_6000_miles + engine_capacity, data = tr_s_high_leverage)
summary(model_reduced_collinearity_FC6000 )
confint(model_reduced_collinearity_FC6000 , level=.95)
res <- cor(my_data, use="pairwise.complete.obs")
round(res, 2)
dev.new()
plot.new()
dev.off()
corrplot(res, type = "upper", order = "hclust",
tl.col = "black", tl.srt = 45, method ="number")
car::vif(model_reduced_collinearity_FC6000 )
fit.linear <- (co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric)
library(glmnet)
attach(tr_s_high_leverage)
x = model.matrix(fit.linear, tr_s_high_leverage)[,-1] #[-1] means no intercept
y = tr_s_high_leverage$co2_emission
grid=10^seq(10,-2,length=100) # Lambda values grid (from 10^10 to 10^-2)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))
ridge.mod$lambda[50] # grid[50] = 11497.57
coef(ridge.mod)[,50] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # l2 norm = 0.33 (DISTANZA DA 0)
ridge.mod$lambda[60] # lambda = 705.48
coef(ridge.mod)[,60] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,60]^2)) # l2 norm = 3.82> l2 for lambda[50]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:11,]
#60 ESIMO VALORE DI LAMBDA
ridge.mod$lambda[60] # lambda = 705.48
coef(ridge.mod)[,60] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,60]^2)) # l2 norm = 3.82> l2 for lambda[50]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:6,]
#60 ESIMO VALORE DI LAMBDA
ridge.mod$lambda[60] # lambda = 705.48
coef(ridge.mod)[,60] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,60]^2)) # l2 norm = 3.82> l2 for lambda[50]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:5,]
# Validation approach to estimate test error
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2) # another typical approach to sample
test=(-train)
y.test=y[test]
# fit a ridge regression model on the training set, and evaluate its MSE on the test set, using lambda = 4.
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,]) # Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
mean((ridge.pred-y.test)^2) # test MSE = 58,59
# PREDICTION WITH LAMBDA= 10^10 , CORRESPONDING TO MODEL WITH ONLY INTERCEPT
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2) #  3206.242
# PREDICTION WITH LAMBDA= 0, CORRESPONDING TO LEAST SQUARE
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train]) # corrected according to errata (glmnet pack updated)
mean((ridge.pred-y.test)^2) #69,41
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min;
bestlam; #5.618761
log(bestlam) # 1.726111
cv.out$lambda.1se
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2) #64,23  #IMPROVEMENT OF MSE RESPECT LAMBDA=4
bestlam
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:5,]
# As expected, none of the coefficients are zero
# ridge regression does not perform variable selection!
dev.new()
plot.new()
plot(out,label = T, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .5)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
dev.new()
plot(lasso.mod,label = T)
dev.new()
plot.new()
plot(lasso.mod,label = T, xvar = "lambda")
# perform cross-validation
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min
print(bestlam) #0.01563454
print(log(bestlam)) #-4.158273
print(cv.out$lambda.1se)
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2)  #56,15
lasso.pred=predict(lasso.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
mean((lasso.pred-y.test)^2)
# However, the lasso has a substantial advantage:
# 1 of the 11 coefficient estimates is exactly zero (extra_urban_metric) .
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:11,]
lasso.coef
lasso.coef[lasso.coef!=0]
cat("Number of coefficients equal to 0:",sum(lasso.coef==0),"\n")
lasso.pred=predict(lasso.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
mean((lasso.pred-y.test)^2)
# However, the lasso has a substantial advantage:
# 1 of the 11 coefficient estimates is exactly zero (extra_urban_metric) .
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:5,]
lasso.coef
lasso.coef[lasso.coef!=0]
cat("Number of coefficients equal to 0:",sum(lasso.coef==0),"\n")
fit.lm=lm(fit.linear <- (co2_emission ~ euro_standard + fuel_cost_6000_miles + fuel_type + engine_capacity + year + transmission_type
+ noise_level + combined_metric + urban_metric ), data=tr_s)
coef(fit.lm)
lasso.coef=predict(out,type="coefficients",s=0)[1:11,]
lasso.coef
# coef(lm(Salary~., data=Hitters)) # differences on the 3rd place
n = nrow(tr_s_high_leverage)
c=nrow(tr_s_high_leverage)
######### Validation Set Approch #########
train=sample(1:n,n)
test=sample(1:c,c)
set.seed(1)
######### Validation Set Approch #########
train=sample(1:n,n)
test=sample(1:c,c)
set.seed(1)
lm.fit=lm(fit.linear, data = data_complete , subset = train)
# the estimated test MSE for the linear regression fit is 33.51284 (seed=1)
y_true=data_complete$co2_emission
y_predict=predict(lm.fit,data_complete)
mean(((y_true-y_predict)[test])^2)
lm.fit=lm(fit.linear, data = tr_s_high_leverage , subset = train)
# the estimated test MSE for the linear regression fit is 33.51284 (seed=1)
y_true=tr_s_high_leverage$co2_emission
y_predict=predict(lm.fit,data_complete)
mean(((y_true-y_predict)[test])^2)
# the estimated test MSE for the linear regression fit is 33.51284 (seed=1)
y_true=tr_s_high_leverage$co2_emission
y_predict=predict(lm.fit,tr_s_high_leverage)
mean(((y_true-y_predict)[test])^2)
# The boot.fn() function can also be used in order to create bootstrap estimates
# for the intercept and slope terms by randomly sampling from among the observations with replacement
# We will compare the estimates obtained using the bootstrap to those obtained using the previous models
library(stringr)
indice=nrow(tr_s_high_leverage)
# No-transformation
set.seed (2)
# No-transformation
set.seed (2)
# No-transformation
set.seed (2)
boot.fn=function(data,index){
return(coef(lm(co2_emission ~  euro_standard + transmission_type +
fuel_type + combined_metric, data = tr_s_high_leverage,subset=index)))
}
boot.fn(tr_s_high_leverage, 1:indice)
# Boot estimate is not deterministic
boot.fn(tr_s_high_leverage,sample(1:n, 45441,replace=T))
boot.fn(tr_s_high_leverage,sample(1:n, 45441,replace=T))
# We use the boot() function to compute the standard errors
# of 1,000 bootstrap estimates for the intercept and slope terms.
b = boot(tr_s_high_leverage ,boot.fn ,1000)
s = summary(lm(fit.linear, data = tr_s_high_leverage))
# Take all std. errors of the bootstrap estimate
x <- capture.output(b)
x <- str_extract(x, "^t[0-9.]+.*$")
x <- x[!is.na(x)]
se <- as.numeric(unlist(str_extract_all(x, '[0-9.]+$')))
# Take all std. errors of the linear model
c = s$coefficients[ ,2]
c = as.numeric(c)
cat("\nDifference between no-Transformation Std.errors:\n",c - se,"\n")
# Polinomials-2 no-linear transformation
set.seed (2)
train.control <- trainControl(method = "cv", number = 10)
set.seed(6)
model_validation <- train(co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric, data = tr_s_high_leverage, method = "lm",
trControl = train.control)
summary(model_validation)
train.control <- trainControl(method = "cv", number = 10)
set.seed(6)
model_validation <- train(co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric, data = tr_s_high_leverage, method = "lm",
trControl = train.control)
summary(model_validation)
confint(model_validation, level=.95)
mean(model_reduced_collinearity_CM_1$residuals)^2)
mean((model_reduced_collinearity_CM_1$residuals)^2)
######### Validation Set Approch #########
train=sample(1:n,n)
test=sample(1:c,c)
set.seed(1)
lm.fit=lm(fit.linear, data = tr_s_high_leverage , subset = train)
# the estimated test MSE for the linear regression fit is 33.51284 (seed=1)
y_true=tr_s_high_leverage$co2_emission
y_predict=predict(lm.fit,tr_s_high_leverage)
mean(((y_true-y_predict)[test])^2)
fit.linear <- (co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric)
fit.linear <- (co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric)
n = nrow(tr_s_high_leverage)
c=nrow(tr_s_high_leverage)
######### Validation Set Approch #########
train=sample(1:n,n)
test=sample(1:c,c)
set.seed(1)
lm.fit=lm(fit.linear, data = tr_s_high_leverage , subset = train)
# the estimated test MSE for the linear regression fit is 33.51284 (seed=1)
y_true=tr_s_high_leverage$co2_emission
y_predict=predict(lm.fit,tr_s_high_leverage)
mean(((y_true-y_predict)[test])^2)
pcr.fit=pcr(fit.linear,data = tr_s_high_leverage,scale=TRUE, validation ="CV")
library(pls)
install.packages("pls")
install.packages("pls")
attach(tr_s_high_leverage)
#################
###### PCR ######
set.seed (2)
pcr.fit=pcr(fit.linear,data = tr_s_high_leverage,scale=TRUE, validation ="CV")
summary(pcr.fit)
pcr.fit=pcr(fit.linear,data = tr_s_high_leverage,scale=TRUE, validation ="CV")
library(pls)
attach(tr_s_high_leverage)
#################
###### PCR ######
set.seed (2)
pcr.fit=pcr(fit.linear,data = tr_s_high_leverage,scale=TRUE, validation ="CV")
summary(pcr.fit)
# Plot the cross-validation scores
# Using val.type="MSEP" will cause the cross-validation MSE to be plotted.
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
which.min(MSEP(pcr.fit)$val[1,,][-1])
# Now perform PCR on the training data and evaluate its test set performance:
set.seed (1)
x = model.matrix(fit.linear,data = tr_s_high_leverage)[,-1]
y = tr_s_high_leverage$co2_emission
train=sample(1:nrow(x), nrow(x)/2) # another typical approach to sample
test=(-train)
y.test=y[test]
pcr.fit=pcr(fit.linear,data = tr_s_high_leverage ,subset=train,scale=TRUE,
validation ="CV")
dev.new()
# Plot MSE and RMSE
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
minPCR <- which.min(MSEP(pcr.fit)$val[1,,][-1]); minPCR # M=8 shows the lowest CV error
dev.new()
plot(RMSEP(pcr.fit),legendpos = "topright")
# We compute the test MSE as follows:
pcr.pred=predict(pcr.fit,x[test,], ncomp=minPCR)
mean((pcr.pred-y.test)^2) # --> 1.095797
# Finally, we fit PCR on the full data set, using M = 8
pcr.fit=pcr(y~x,scale=TRUE,ncomp=which.min(MSEP(pcr.fit)$val[1,,][-1]))
mean((pcr.pred-y.test)^2) # --> 8.00
# Finally, we fit PCR on the full data set, using M = 8
pcr.fit=pcr(y~x,scale=TRUE,ncomp=which.min(MSEP(pcr.fit)$val[1,,][-1]))
summary(pcr.fit)
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
dev.new()
plot(pcr.fit, ncomp = 8, asp = 1, line = TRUE)
coef(pcr.fit) ## get the coefficients
######################
######### PLS ########
set.seed (1)
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = tr_s_high_leverage, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 8
# Now perform Pls on the training data and evaluate its test set performance:
set.seed (1)
pls.fit=plsr(fit.linear,data = tr_s_high_leverage, subset=train,
scale=TRUE, validation ="CV")
validationplot(pls.fit,val.type="MSEP");
which.min(MSEP(pls.fit)$val[1,,][-1]); # M = 6
pls.pred=predict(pls.fit,x[test,],ncomp=8)
pls.pred=predict(pls.fit,x[test,],ncomp=4)
mean((pls.pred-y.test)^2) # --> 34.44799
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = tr_s_high_leverage ,scale=TRUE,ncomp=8)
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = tr_s_high_leverage ,scale=TRUE,ncomp=4)
summary(pls.fit)
