dev.new()
plot(RMSEP(pcr.fit),legendpos = "topright")
# We compute the test MSE as follows:
pcr.pred=predict(pcr.fit,x[test,], ncomp=minPCR)
mean((pcr.pred-y.test)^2)
pcr.fit=pcr(y~x,scale=TRUE,ncomp=which.min(MSEP(pcr.fit)$val[1,,][-1]))
summary(pcr.fit)
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
dev.new()
plot(pcr.fit, ncomp = which.min(MSEP(pcr.fit)$val[1,,][-1]), asp = 1, line = TRUE)
pcr.fit$coefficients ## get the coefficients
######################
######### PLS ########
set.seed (1)
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = data_complete_2, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 8
# Now perform Pls on the training data and evaluate its test set performance:
set.seed (1)
pls.fit=plsr(fit.linear,data = data_complete_2, subset=train,
scale=TRUE, validation ="CV")
validationplot(pls.fit,val.type="MSEP");
which.min(MSEP(pls.fit)$val[1,,][-1]);
######## PLS ########
set.seed (1)
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = data_complete_2, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 8
pcr.fit=pcr(y~x,scale=TRUE,ncomp=which.min(MSEP(pcr.fit)$val[1,,][-1]))
summary(pcr.fit)
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
dev.new()
plot(pcr.fit, ncomp = which.min(MSEP(pcr.fit)$val[1,,][-1]), asp = 1, line = TRUE)
pcr.fit$coefficients ## get the coefficients
pls.fit=plsr(fit.linear,data = data_complete_2, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 8
######################
######### PLS ########
set.seed (1)
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = data_complete_2, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 8
# Now perform Pls on the training data and evaluate its test set performance:
set.seed (1)
pls.fit=plsr(fit.linear,data = data_complete_2, subset=train,
scale=TRUE, validation ="CV")
validationplot(pls.fit,val.type="MSEP");
which.min(MSEP(pls.fit)$val[1,,][-1]);
######################
######### PLS ########
set.seed (1)
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = data_complete_2, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 8
# Now perform Pls on the training data and evaluate its test set performance:
set.seed (1)
pls.fit=plsr(fit.linear,data = data_complete_2, subset=train,
scale=TRUE, validation ="CV")
dev.new()
validationplot(pls.fit,val.type="MSEP");
which.min(MSEP(pls.fit)$val[1,,][-1]);
pls.pred=predict(pls.fit,x[test,],ncomp=5)
pls.pred=predict(pls.fit,x[test,],ncomp=5)
mean((pls.pred-y.test)^2)
######################
######### PLS ########
set.seed (1)
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = data_complete_2, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 8
# Now perform Pls on the training data and evaluate its test set performance:
set.seed (1)
pls.fit=plsr(fit.linear,data = data_complete_2, subset=train,
scale=TRUE, validation ="CV")
dev.new()
validationplot(pls.fit,val.type="MSEP");
which.min(MSEP(pls.fit)$val[1,,][-1]);
pls.pred=predict(pls.fit,x[test,],ncomp=5)
mean((pls.pred-y.test)^2)
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = data_complete_2 ,scale=TRUE,ncomp=9)
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = data_complete_2 ,scale=TRUE,ncomp=9)
summary(pls.fit)
pls.pred=predict(pls.fit,x[test,],ncomp=7)
mean((pls.pred-y.test)^2)
pls.pred=predict(pls.fit,x[test,],ncomp=9)
mean((pls.pred-y.test)^2)
pls.pred=predict(pls.fit,x[test,],ncomp=5)
mean((pls.pred-y.test)^2)
pls.pred=predict(pls.fit,x[test,],ncomp=5)
mean((pls.pred-y.test)^2)
pls.pred=predict(pls.fit,x[test,],ncomp=9)
mean((pls.pred-y.test)^2)
######################
######### PLS ########
set.seed (1)
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = data_complete_2, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 8
# Now perform Pls on the training data and evaluate its test set performance:
set.seed (1)
pls.fit=plsr(fit.linear,data = data_complete_2, subset=train,
scale=TRUE, validation ="CV")
dev.new()
validationplot(pls.fit,val.type="MSEP");
which.min(MSEP(pls.fit)$val[1,,][-1]);
pls.pred=predict(pls.fit,x[test,],ncomp=9)
mean((pls.pred-y.test)^2)
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = data_complete_2 ,scale=TRUE,ncomp=9)
summary(pls.fit)
pls.fit=plsr(fit.linear,data = data_complete_2 ,scale=TRUE,ncomp=5)
summary(pls.fit)
pls.pred=predict(pls.fit,x[test,],ncomp=5)
mean((pls.pred-y.test)^2)
pls.fit=plsr(fit.linear,data = data_complete_2 ,scale=TRUE,ncomp=6)
summary(pls.fit)
pls.pred=predict(pls.fit,x[test,],ncomp=5)
mean((pls.pred-y.test)^2)
pls.pred=predict(pls.fit,x[test,],ncomp=9)
mean((pls.pred-y.test)^2)
pls.pred=predict(pls.fit,x[test,],ncomp=9)
######################
######### PLS ########
set.seed (1)
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = data_complete_2, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 8
# Now perform Pls on the training data and evaluate its test set performance:
set.seed (1)
pls.fit=plsr(fit.linear,data = data_complete_2, subset=train,
scale=TRUE, validation ="CV")
dev.new()
validationplot(pls.fit,val.type="MSEP");
which.min(MSEP(pls.fit)$val[1,,][-1]);
pls.pred=predict(pls.fit,x[test,],ncomp=9)
mean((pls.pred-y.test)^2)
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = data_complete_2 ,scale=TRUE,ncomp=9)
summary(pls.fit)
library(pls)
attach(data_complete_2)
fit.linear <- (co2_emission ~ year + euro_standard + transmission_type +
fuel_type + combined_metric  + noise_level +urban_metric +extra_urban_metric+fuel_cost_6000_miles)
#################
###### PCR ######
set.seed (2)
pcr.fit=pcr(fit.linear,data = data_complete_2,scale=TRUE, validation ="CV")
summary(pcr.fit)
# Plot the cross-validation scores
# Using val.type="MSEP" will cause the cross-validation MSE to be plotted.
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
which.min(MSEP(pcr.fit)$val[1,,][-1])
# Now perform PCR on the training data and evaluate its test set performance:
set.seed (1)
x = model.matrix(fit.linear,data = data_complete_2)[,-1]
y = data_complete_2$co2_emission
n = nrow(data_complete_2)
train=sample(1:n,n/2)
test=(-train)
y.test=y[test]
pcr.fit=pcr(fit.linear,data = data_complete_2 ,subset=train,scale=TRUE,
validation ="CV")
dev.new()
# Plot MSE and RMSE
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
minPCR <- which.min(MSEP(pcr.fit)$val[1,,][-1]); minPCR
dev.new()
plot(RMSEP(pcr.fit),legendpos = "topright")
# We compute the test MSE as follows:
pcr.pred=predict(pcr.fit,x[test,], ncomp=minPCR)
mean((pcr.pred-y.test)^2)
fit.linear <- (co2_emission ~ year + euro_standard + transmission_type +
fuel_type + combined_metric  + noise_level +urban_metric +extra_urban_metric+fuel_cost_6000_miles)
#################
###### PCR ######
set.seed (2)
pcr.fit=pcr(fit.linear,data = data_complete_2,scale=TRUE, validation ="CV")
summary(pcr.fit)
# Plot the cross-validation scores
# Using val.type="MSEP" will cause the cross-validation MSE to be plotted.
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
which.min(MSEP(pcr.fit)$val[1,,][-1])
# Now perform PCR on the training data and evaluate its test set performance:
set.seed (1)
x = model.matrix(fit.linear,data = data_complete_2)[,-1]
y = data_complete_2$co2_emission
n = nrow(data_complete_2)
train=sample(1:n,n/2)
test=(-train)
y.test=y[test]
pcr.fit=pcr(fit.linear,data = data_complete_2 ,subset=train,scale=TRUE,
validation ="CV")
dev.new()
# Plot MSE and RMSE
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
minPCR <- which.min(MSEP(pcr.fit)$val[1,,][-1]); minPCR
dev.new()
library(pls)
attach(tr_s_outliers)
fit.linear <-(co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric  + noise_level)
#################
###### PCR ######
set.seed (2)
pcr.fit=pcr(fit.linear,data = tr_s_outliers,scale=TRUE, validation ="CV")
summary(pcr.fit)
# Plot the cross-validation scores
# Using val.type="MSEP" will cause the cross-validation MSE to be plotted.
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
which.min(MSEP(pcr.fit)$val[1,,][-1])
# Now perform PCR on the training data and evaluate its test set performance:
set.seed (1)
# Now perform PCR on the training data and evaluate its test set performance:
set.seed (1)
x = model.matrix(fit.linear,data = tr_s_outliers)[,-1]
y = tr_s_outliers$co2_emission
n = nrow(tr_s_outliers)
train=sample(1:n,n/2)
test=(-train)
y.test=y[test]
pcr.fit=pcr(fit.linear,data = tr_s_outliers ,subset=train,scale=TRUE,
validation ="CV")
dev.new()
# Plot MSE and RMSE
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
minPCR <- which.min(MSEP(pcr.fit)$val[1,,][-1]); minPCR
dev.new()
plot(RMSEP(pcr.fit),legendpos = "topright")
# We compute the test MSE as follows:
pcr.pred=predict(pcr.fit,x[test,], ncomp=minPCR)
mean((pcr.pred-y.test)^2)
pcr.fit=pcr(y~x,scale=TRUE,ncomp=which.min(MSEP(pcr.fit)$val[1,,][-1]))
summary(pcr.fit)
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
dev.new()
plot(pcr.fit, ncomp = which.min(MSEP(pcr.fit)$val[1,,][-1]), asp = 1, line = TRUE)
pcr.fit$coefficients ## get the coefficients
######################
######### PLS ########
set.seed (1)
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = data_complete_2, scale=TRUE,
validation ="CV")
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = tr_s_outliers, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 8
# Now perform Pls on the training data and evaluate its test set performance:
set.seed (1)
pls.fit=plsr(fit.linear,data = tr_s_outliers, subset=train,
scale=TRUE, validation ="CV")
dev.new()
validationplot(pls.fit,val.type="MSEP");
which.min(MSEP(pls.fit)$val[1,,][-1]);
pls.pred=predict(pls.fit,x[test,],ncomp=5)
mean((pls.pred-y.test)^2)
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = data_complete_2 ,scale=TRUE,ncomp=9)
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = data_complete_2 ,scale=TRUE,ncomp=5)
summary(pls.fit)
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = tr_s_outliers ,scale=TRUE,ncomp=5)
summary(pls.fit)
library(pls)
attach(data_complete_2)
fit.linear <- (co2_emission ~ year + euro_standard + transmission_type +
fuel_type + combined_metric  + noise_level +urban_metric +extra_urban_metric+fuel_cost_6000_miles)
#################
###### PCR ######
set.seed (2)
pcr.fit=pcr(fit.linear,data = data_complete_2,scale=TRUE, validation ="CV")
summary(pcr.fit)
# Plot the cross-validation scores
# Using val.type="MSEP" will cause the cross-validation MSE to be plotted.
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
which.min(MSEP(pcr.fit)$val[1,,][-1])
# Now perform PCR on the training data and evaluate its test set performance:
set.seed (1)
x = model.matrix(fit.linear,data = data_complete_2)[,-1]
y = data_complete_2$co2_emission
n = nrow(data_complete_2)
train=sample(1:n,n/2)
test=(-train)
y.test=y[test]
pcr.fit=pcr(fit.linear,data = data_complete_2 ,subset=train,scale=TRUE,
validation ="CV")
dev.new()
# Plot MSE and RMSE
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
minPCR <- which.min(MSEP(pcr.fit)$val[1,,][-1]); minPCR
dev.new()
plot(RMSEP(pcr.fit),legendpos = "topright")
# We compute the test MSE as follows:
pcr.pred=predict(pcr.fit,x[test,], ncomp=minPCR)
mean((pcr.pred-y.test)^2)
minPCR
pcr.fit=pcr(y~x,scale=TRUE,ncomp=which.min(MSEP(pcr.fit)$val[1,,][-1]))
summary(pcr.fit)
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
dev.new()
dev.new()
plot(pcr.fit, ncomp = which.min(MSEP(pcr.fit)$val[1,,][-1]), asp = 1, line = TRUE)
pcr.fit$coefficients ## get the coefficients
######################
######### PLS ########
set.seed (1)
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = data_complete_2, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 8
# Now perform Pls on the training data and evaluate its test set performance:
set.seed (1)
pls.fit=plsr(fit.linear,data = data_complete_2, subset=train,
scale=TRUE, validation ="CV")
which.min(MSEP(pls.fit)$val[1,,][-1]);
pls.pred=predict(pls.fit,x[test,],ncomp=which.min(MSEP(pls.fit)$val[1,,][-1]))
mean((pls.pred-y.test)^2)
pls.pred=predict(pls.fit,x[test,],ncomp=6)
mean((pls.pred-y.test)^2)
pls.pred=predict(pls.fit,x[test,],ncomp=9)
mean((pls.pred-y.test)^2)
library(glmnet)
attach(data_complete_2)
fit.linear <- (co2_emission ~ year + euro_standard + transmission_type +
fuel_type + combined_metric  + noise_level +urban_metric +extra_urban_metric+fuel_cost_6000_miles)
x = model.matrix(fit.linear, data_complete_2)[,-1] #[-1] means no intercept
y = data_complete_2$co2_emission
grid=10^seq(10,-2,length=100) # Lambda values grid (from 10^10 to 10^-2)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
#Associated with each value of lambda is a vector of ridge regression
#coefficients, stored in a matrix that can be accessed by coef(),
#in this case 12x100, 11+intercept for each lambda value:
dim(coef(ridge.mod))
# We expect the coefficient estimates to be much smaller, in terms of l2 norm,
#when a large value of lambda is used, as compared to when a small value is used.
ridge.mod$lambda[50] # grid[50] = 11497.57
coef(ridge.mod)[,50] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # l2 norm = 0.33 (DISTANZA DA 0)
#60 ESIMO VALORE DI LAMBDA
ridge.mod$lambda[60] # lambda = 705.48
coef(ridge.mod)[,60] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,60]^2)) # l2 norm = 3.82> l2 for lambda[50]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:10,]
# Validation approach to estimate test error
set.seed(1)
n = nrow(data_complete_2)
train=sample(1:n,n/2)
test=(-train)
y.test=y[test]
# fit a ridge regression model on the training set, and evaluate its MSE on the test set, using lambda = 4.
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,]) # Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
mean((ridge.pred-y.test)^2)
mean((mean(y[train ])-y.test)^2) # test MSE, if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations.
# PREDICTION WITH LAMBDA= 10^10 , CORRESPONDING TO MODEL WITH ONLY INTERCEPT
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)
# PREDICTION WITH LAMBDA= 0, CORRESPONDING TO LEAST SQUARE
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train]) # corrected according to errata (glmnet pack updated)
mean((ridge.pred-y.test)^2)
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
dev.new()
bestlam=cv.out$lambda.min;
bestlam;
log(bestlam)
cv.out$lambda.1se
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2)  #IMPROVEMENT OF MSE RESPECT LAMBDA=4
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
dev.new()
plot(lasso.mod,label = T)
dev.new()
plot.new()
plot(lasso.mod,label = T, xvar = "lambda")
# perform cross-validation
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min
print(bestlam)
print(log(bestlam))
print(cv.out$lambda.1se)
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2)
pls.pred=predict(pls.fit,x[test,],ncomp=9)
mean((pls.pred-y.test)^2)
# We compute the test MSE as follows:
pcr.pred=predict(pcr.fit,x[test,], ncomp=minPCR)
mean((pcr.pred-y.test)^2)
library(pls)
attach(data_complete_2)
fit.linear <- (co2_emission ~ year + euro_standard + transmission_type +
fuel_type + combined_metric  + noise_level +urban_metric +extra_urban_metric+fuel_cost_6000_miles)
#################
###### PCR ######
set.seed (2)
pcr.fit=pcr(fit.linear,data = data_complete_2,scale=TRUE, validation ="CV")
summary(pcr.fit)
# Plot the cross-validation scores
# Using val.type="MSEP" will cause the cross-validation MSE to be plotted.
dev.new()
which.min(MSEP(pcr.fit)$val[1,,][-1])
# Now perform PCR on the training data and evaluate its test set performance:
set.seed (1)
x = model.matrix(fit.linear,data = data_complete_2)[,-1]
y = data_complete_2$co2_emission
n = nrow(data_complete_2)
train=sample(1:n,n/2)
test=(-train)
y.test=y[test]
pcr.fit=pcr(fit.linear,data = data_complete_2 ,subset=train,scale=TRUE,
validation ="CV")
minPCR <- which.min(MSEP(pcr.fit)$val[1,,][-1]); minPCR
# We compute the test MSE as follows:
pcr.pred=predict(pcr.fit,x[test,], ncomp=minPCR)
mean((pcr.pred-y.test)^2)
minPCR
# We compute the test MSE as follows:
pcr.pred=predict(pcr.fit,x[test,], ncomp=9)
mean((pcr.pred-y.test)^2)
