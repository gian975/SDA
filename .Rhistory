c = as.numeric(c)
cat("\nDifference between poly-2 transformation Std.errors:\n",c - se,"\n")
s
b
# Log trasformation no-linear transformation
set.seed (2)
indice=nrow(tr_s)
model_not_linear_log <- lm(co2_emission ~ euro_standard + transmission_type + log(engine_capacity) +
fuel_type + combined_metric  + noise_level, data = tr_s)
boot.fn=function(data,index){
return(coef(lm(co2_emission ~ euro_standard + transmission_type + log(engine_capacity) +
fuel_type + combined_metric  + noise_level,data = data,subset=index)))
}
boot.fn(tr_s, 1:indice)
b = boot(tr_s ,boot.fn ,1000)
s = summary(lm(model_not_linear_log,data = tr_s))
b
# Take all std. errors of the bootstrap estimate
x <- capture.output(b)
x <- str_extract(x, "^t[0-9.]+.*$")
x <- x[!is.na(x)]
se <- as.numeric(unlist(str_extract_all(x, '[0-9.]+$')))
# Take all std. errors of the log transformation
c = s$coefficients[ ,2]
c = as.numeric(c)
cat("\nDifference between poly-2 transformation Std.errors:\n",c - se,"\n")
# Sqrt trasformation no-linear transformation
set.seed (2)
indice=nrow(tr_s)
model_not_linear_sqrt <- lm(co2_emission ~ euro_standard + transmission_type + sqrt(engine_capacity) +
fuel_type + combined_metric  + noise_level, data = tr_s)
boot.fn=function(data,index){
return(coef(lm(co2_emission ~ euro_standard + transmission_type + sqrt(engine_capacity) +
fuel_type + combined_metric  + noise_level,data = data,subset=index)))
}
boot.fn(tr_s, 1:indice)
boot.fn(tr_s,sample(1:indice, 45441,replace=T))
b = boot(tr_s ,boot.fn ,1000)
s = summary(lm(model_not_linear_sqrt,data = tr_s))
b
s
# Take all std. errors of the bootstrap estimate
x <- capture.output(b)
x <- str_extract(x, "^t[0-9.]+.*$")
x <- x[!is.na(x)]
se <- as.numeric(unlist(str_extract_all(x, '[0-9.]+$')))
# Take all std. errors of the log transformation
c = s$coefficients[ ,2]
c = as.numeric(c)
cat("\nDifference between poly-2 transformation Std.errors:\n",c - se,"\n")
# Log trasformation no-linear transformation
set.seed (2)
indice=nrow(tr_s)
model_not_linear_log <- lm(co2_emission ~ euro_standard + transmission_type + log(engine_capacity) +
fuel_type + combined_metric  + noise_level, data = tr_s)
boot.fn=function(data,index){
return(coef(lm(co2_emission ~ euro_standard + transmission_type + log(engine_capacity) +
fuel_type + combined_metric  + noise_level,data = data,subset=index)))
}
boot.fn(tr_s, 1:indice)
b = boot(tr_s ,boot.fn ,1000)
b
s
s = summary(lm(model_not_linear_log,data = tr_s))
# Take all std. errors of the bootstrap estimate
x <- capture.output(b)
x <- str_extract(x, "^t[0-9.]+.*$")
x <- x[!is.na(x)]
se <- as.numeric(unlist(str_extract_all(x, '[0-9.]+$')))
# Take all std. errors of the log transformation
c = s$coefficients[ ,2]
c = as.numeric(c)
cat("\nDifference between log transformation Std.errors:\n",c - se,"\n")
library(glmnet)
attach(tr_s_outliers)
fit.linear <- (co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric  + noise_level)
x = model.matrix(fit.linear, tr_s_outliers)[,-1] #[-1] means no intercept
y = tr_s_outliers$co2_emission
grid=10^seq(10,-2,length=100) # Lambda values grid (from 10^10 to 10^-2)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
#Associated with each value of lambda is a vector of ridge regression
#coefficients, stored in a matrix that can be accessed by coef(),
#in this case 12x100, 11+intercept for each lambda value:
dim(coef(ridge.mod))
# We expect the coefficient estimates to be much smaller, in terms of l2 norm,
#when a large value of lambda is used, as compared to when a small value is used.
ridge.mod$lambda[50] # grid[50] = 11497.57
coef(ridge.mod)[,50] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # l2 norm = 0.33 (DISTANZA DA 0)
#60 ESIMO VALORE DI LAMBDA
ridge.mod$lambda[60] # lambda = 705.48
coef(ridge.mod)[,60] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,60]^2)) # l2 norm = 3.82> l2 for lambda[50]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:5,]
# Validation approach to estimate test error
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2) # another typical approach to sample
test=(-train)
y.test=y[test]
# fit a ridge regression model on the training set, and evaluate its MSE on the test set, using lambda = 4.
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,]) # Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
mean((ridge.pred-y.test)^2) # test MSE = 33.44
mean((mean(y[train ])-y.test)^2) # test MSE, if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations.
# PREDICTION WITH LAMBDA= 10^10 , CORRESPONDING TO MODEL WITH ONLY INTERCEPT
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2) #  3206.242
# PREDICTION WITH LAMBDA= 0, CORRESPONDING TO LEAST SQUARE
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train]) # corrected according to errata (glmnet pack updated)
mean((ridge.pred-y.test)^2) #69,41
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min;
bestlam; #5.618761
log(bestlam) # 1.726111
cv.out$lambda.1se
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2) #64,23  #IMPROVEMENT OF MSE RESPECT LAMBDA=4
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:5,]
# As expected, none of the coefficients are zero
# ridge regression does not perform variable selection!
dev.new()
plot.new()
plot(out,label = T, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .5)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
dev.new()
plot(lasso.mod,label = T)
dev.new()
plot.new()
plot(lasso.mod,label = T, xvar = "lambda")
# perform cross-validation
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min
print(bestlam) #0.01563454
print(log(bestlam)) #-4.158273
print(cv.out$lambda.1se)
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2)  #56,15
lasso.pred=predict(lasso.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
mean((lasso.pred-y.test)^2)
# However, the lasso has a substantial advantage:
# 1 of the 11 coefficient estimates is exactly zero (extra_urban_metric) .
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:5,]
lasso.coef
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:6,]
lasso.coef
lasso.coef[lasso.coef!=0]
cat("Number of coefficients equal to 0:",sum(lasso.coef==0),"\n")
fit.lm=lm(fit.linear <- (co2_emission ~ euro_standard + fuel_cost_6000_miles + fuel_type + engine_capacity + year + transmission_type
+ noise_level + combined_metric + urban_metric ), data=tr_s)
library(pls)
attach(tr_s_outliers)
#################
###### PCR ######
set.seed (2)
pcr.fit=pcr(fit.linear,data = tr_s_outliers,scale=TRUE, validation ="CV")
summary(pcr.fit)
# Plot the cross-validation scores
# Using val.type="MSEP" will cause the cross-validation MSE to be plotted.
dev.new()
pcr.fit=pcr(fit.linear,data = tr_s_outliers,scale=TRUE, validation ="CV")
fit.linear
fit.linear <- (co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric  + noise_level)
#################
###### PCR ######
set.seed (2)
pcr.fit=pcr(fit.linear,data = tr_s_outliers,scale=TRUE, validation ="CV")
summary(pcr.fit)
# Plot the cross-validation scores
# Using val.type="MSEP" will cause the cross-validation MSE to be plotted.
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
which.min(MSEP(pcr.fit)$val[1,,][-1])
# Now perform PCR on the training data and evaluate its test set performance:
set.seed (1)
x = model.matrix(fit.linear,data = tr_s_high_leverage)[,-1]
# Plot the cross-validation scores
# Using val.type="MSEP" will cause the cross-validation MSE to be plotted.
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
which.min(MSEP(pcr.fit)$val[1,,][-1])
# Now perform PCR on the training data and evaluate its test set performance:
set.seed (1)
x = model.matrix(fit.linear,data = tr_s_outliers)[,-1]
y = tr_s_outliers$co2_emission
train=sample(1:nrow(x), nrow(x)/2) # another typical approach to sample
test=(-train)
y.test=y[test]
pcr.fit=pcr(fit.linear,data = tr_s_outliers ,subset=train,scale=TRUE,
validation ="CV")
dev.new()
# Plot MSE and RMSE
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
minPCR <- which.min(MSEP(pcr.fit)$val[1,,][-1]); minPCR # M=8 shows the lowest CV error
dev.new()
plot(RMSEP(pcr.fit),legendpos = "topright")
# We compute the test MSE as follows:
pcr.pred=predict(pcr.fit,x[test,], ncomp=minPCR)
mean((pcr.pred-y.test)^2) # --> 8.00
# Finally, we fit PCR on the full data set, using M = 8
pcr.fit=pcr(y~x,scale=TRUE,ncomp=which.min(MSEP(pcr.fit)$val[1,,][-1]))
summary(pcr.fit)
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
dev.new()
plot(pcr.fit, ncomp = 8, asp = 1, line = TRUE)
coef(pcr.fit) ## get the coefficients
######################
######### PLS ########
set.seed (1)
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = tr_s_outliers, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 8
# Now perform Pls on the training data and evaluate its test set performance:
set.seed (1)
# Now perform Pls on the training data and evaluate its test set performance:
set.seed (1)
pls.fit=plsr(fit.linear,data = tr_s_outliers, subset=train,
scale=TRUE, validation ="CV")
validationplot(pls.fit,val.type="MSEP");
which.min(MSEP(pls.fit)$val[1,,][-1]); # M = 6
pls.pred=predict(pls.fit,x[test,],ncomp=4)
mean((pls.pred-y.test)^2) # --> 8.00
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = tr_s_high_leverage ,scale=TRUE,ncomp=4)
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = tr_s_outliers ,scale=TRUE,ncomp=4)
summary(pls.fit)
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = tr_s_outliers ,scale=TRUE,ncomp=5)
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP");
which.min(MSEP(pls.fit)$val[1,,][-1]); # M = 6
pls.pred=predict(pls.fit,x[test,],ncomp=5)
mean((pls.pred-y.test)^2) # --> 8.00
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = tr_s_outliers ,scale=TRUE,ncomp=5)
x = model.matrix(fit.linear,data = tr_s_outliers)[,-1]
y = tr_s_outliers$co2_emission
train=sample(1:nrow(x), nrow(x)) # another typical approach to sample
test=tr_s
y.test=y[test]
t_s_1=t_s[, c(1,2,3,5,6,7,8,9,10,11)]
View(tr_s_1)
View(tr_s_1)
View(t_s_1)
View(t_s_1)
# Now perform PCR on the training data and evaluate its test set performance:
set.seed (1)
x = model.matrix(fit.linear,data = tr_s_outliers)[,-1]
y = tr_s_outliers$co2_emission
n = nrow(tr_s_outliers)
c=nrow(t_s_1)
train=sample(1:n,n)
test=sample(1:c,c)
y.test=y[test]
pcr.fit=pcr(fit.linear,data = tr_s_outliers ,subset=train,scale=TRUE,
validation ="CV")
dev.new()
# Plot MSE and RMSE
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
minPCR <- which.min(MSEP(pcr.fit)$val[1,,][-1]); minPCR # M=8 shows the lowest CV error
dev.new()
plot(RMSEP(pcr.fit),legendpos = "topright")
# We compute the test MSE as follows:
pcr.pred=predict(pcr.fit,x[test,], ncomp=minPCR)
mean((pcr.pred-y.test)^2) # --> 8.00
# Finally, we fit PCR on the full data set, using M = 8
pcr.fit=pcr(y~x,scale=TRUE,ncomp=which.min(MSEP(pcr.fit)$val[1,,][-1]))
summary(pcr.fit)
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
dev.new()
plot(pcr.fit, ncomp = 8, asp = 1, line = TRUE)
coef(pcr.fit) ## get the coefficients
# Finally, we fit PCR on the full data set, using M = 8
pcr.fit=pcr(y~x,scale=TRUE,ncomp=which.min(MSEP(pcr.fit)$val[1,,][-1]))
'summary(pcr.fit)
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
dev.new()
plot(pcr.fit, ncomp = 5, asp = 1, line = TRUE)
coef(pcr.fit) ## get the coefficients
######################
######### PLS ########
set.seed (1)
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = tr_s_outliers, scale=TRUE,
validation ="CV")
# Finally, we fit PCR on the full data set, using M = 8
pcr.fit=pcr(y~x,scale=TRUE,ncomp=which.min(MSEP(pcr.fit)$val[1,,][-1]))
summary(pcr.fit)
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
dev.new()
plot(pcr.fit, ncomp = 5, asp = 1, line = TRUE)
coef(pcr.fit) ## get the coefficients
######################
######### PLS ########
set.seed (1)
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = tr_s_outliers, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 8
# Now perform Pls on the training data and evaluate its test set performance:
set.seed (1)
pls.fit=plsr(fit.linear,data = tr_s_outliers, subset=train,
scale=TRUE, validation ="CV")
validationplot(pls.fit,val.type="MSEP");
which.min(MSEP(pls.fit)$val[1,,][-1]); # M = 6
pls.pred=predict(pls.fit,x[test,],ncomp=5)
mean((pls.pred-y.test)^2) # --> 8.00
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = tr_s_outliers ,scale=TRUE,ncomp=5)
summary(pls.fit)
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
train=sample(1:n,n/2)
test=sample(1:c,c)
y.test=y[test]
pcr.fit=pcr(fit.linear,data = tr_s_outliers ,subset=train,scale=TRUE,
validation ="CV")
dev.new()
# Plot MSE and RMSE
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
minPCR <- which.min(MSEP(pcr.fit)$val[1,,][-1]); minPCR # M=8 shows the lowest CV error
dev.new()
plot(RMSEP(pcr.fit),legendpos = "topright")
# We compute the test MSE as follows:
pcr.pred=predict(pcr.fit,x[test,], ncomp=minPCR)
mean((pcr.pred-y.test)^2) # --> 8.00
# Plot MSE and RMSE
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
minPCR <- which.min(MSEP(pcr.fit)$val[1,,][-1]); minPCR # M=8 shows the lowest CV error
dev.new()
plot(RMSEP(pcr.fit),legendpos = "topright")
# We compute the test MSE as follows:
pcr.pred=predict(pcr.fit,x[test,], ncomp=minPCR)
mean((pcr.pred-y.test)^2) # --> 8.00
# Finally, we fit PCR on the full data set, using M = 8
pcr.fit=pcr(y~x,scale=TRUE,ncomp=which.min(MSEP(pcr.fit)$val[1,,][-1]))
summary(pcr.fit)
dev.new()
validationplot(pcr.fit,val.type="MSEP",legendpos = "topright")
dev.new()
plot(pcr.fit, ncomp = 5, asp = 1, line = TRUE)
coef(pcr.fit) ## get the coefficients
######################
######### PLS ########
set.seed (1)
# Using the plsr() function:
# Pls is similar to pcr, but is used to manage the variance of y
# With pls, we don't concentrate our attention only on the regressions, but also the y variable
# Setting scale=TRUE has the effect of standardizing each predictor (scale projection).
# Setting validation = 'CV' has the effect to use cross validation to rate M parameter
pls.fit=plsr(fit.linear,data = tr_s_outliers, scale=TRUE,
validation ="CV")
summary(pls.fit)
dev.new()
validationplot(pls.fit,val.type="MSEP")
which.min(MSEP(pls.fit)$val[1,,][-1]) # M = 8
# Now perform Pls on the training data and evaluate its test set performance:
set.seed (1)
pls.fit=plsr(fit.linear,data = tr_s_outliers, subset=train,
scale=TRUE, validation ="CV")
validationplot(pls.fit,val.type="MSEP");
which.min(MSEP(pls.fit)$val[1,,][-1]); # M = 6
pls.pred=predict(pls.fit,x[test,],ncomp=5)
mean((pls.pred-y.test)^2) # --> 8.00
# Finally, we perform PLS using the full data set, using M = 4,
pls.fit=plsr(fit.linear,data = tr_s_outliers ,scale=TRUE,ncomp=5)
summary(pls.fit)
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
#Final result: with 4 components we can explain the same variance of y obtained with 8 components
fit.linear <- (co2_emission ~ euro_standard + transmission_type +
fuel_type + combined_metric  + noise_level)
x = model.matrix(fit.linear, tr_s_outliers)[,-1] #[-1] means no intercept
y = tr_s_outliers$co2_emission
grid=10^seq(10,-2,length=100) # Lambda values grid (from 10^10 to 10^-2)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
#Associated with each value of lambda is a vector of ridge regression
#coefficients, stored in a matrix that can be accessed by coef(),
#in this case 12x100, 11+intercept for each lambda value:
dim(coef(ridge.mod))
# We expect the coefficient estimates to be much smaller, in terms of l2 norm,
#when a large value of lambda is used, as compared to when a small value is used.
ridge.mod$lambda[50] # grid[50] = 11497.57
coef(ridge.mod)[,50] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # l2 norm = 0.33 (DISTANZA DA 0)
#60 ESIMO VALORE DI LAMBDA
ridge.mod$lambda[60] # lambda = 705.48
coef(ridge.mod)[,60] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,60]^2)) # l2 norm = 3.82> l2 for lambda[50]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:5,]
# Validation approach to estimate test error
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2) # another typical approach to sample
n = nrow(tr_s_outliers)
c=nrow(t_s_1)
train=sample(1:n,n/2)
test=sample(1:c,c)
y.test=y[test]
# fit a ridge regression model on the training set, and evaluate its MSE on the test set, using lambda = 4.
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,]) # Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
mean((ridge.pred-y.test)^2) # test MSE = 33.44
mean((mean(y[train ])-y.test)^2) # test MSE, if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations.
# PREDICTION WITH LAMBDA= 10^10 , CORRESPONDING TO MODEL WITH ONLY INTERCEPT
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2) #  3206.242
# PREDICTION WITH LAMBDA= 0, CORRESPONDING TO LEAST SQUARE
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train]) # corrected according to errata (glmnet pack updated)
mean((ridge.pred-y.test)^2) #69,41
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min;
bestlam; #5.618761
log(bestlam) # 1.726111
cv.out$lambda.1se
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2) #64,23  #IMPROVEMENT OF MSE RESPECT LAMBDA=4
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:5,]
# As expected, none of the coefficients are zero
# ridge regression does not perform variable selection!
dev.new()
plot.new()
plot(out,label = T, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .5)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
dev.new()
plot(lasso.mod,label = T)
dev.new()
plot.new()
plot(lasso.mod,label = T, xvar = "lambda")
# perform cross-validation
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min
print(bestlam) #0.01563454
print(log(bestlam)) #-4.158273
print(cv.out$lambda.1se)
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2)  #56,15
lasso.pred=predict(lasso.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
mean((lasso.pred-y.test)^2)
# However, the lasso has a substantial advantage:
# 1 of the 11 coefficient estimates is exactly zero (extra_urban_metric) .
out=glmnet(x,y,alpha=1,lambda=grid)
set.seed (1)
pls.fit=plsr(fit.linear,data = tr_s_outliers, subset=train,
scale=TRUE, validation ="CV")
validationplot(pls.fit,val.type="MSEP");
which.min(MSEP(pls.fit)$val[1,,][-1]); # M = 6
pls.pred=predict(pls.fit,x[test,],ncomp=5)
mean((pls.pred-y.test)^2) # --> 8.00
