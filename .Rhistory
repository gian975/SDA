#when a large value of lambda is used, as compared to when a small value is used.
ridge.mod$lambda[50] # grid[50] = 11497.57
e
coef(ridge.mod)[,50] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # l2 norm
#60 ESIMO VALORE DI LAMBDA
ridge.mod$lambda[60] # lambda = 705.48
ridge.mod$lambda[60] # lambda = 705.48
coef(ridge.mod)[,60] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,60]^2)) # l2 norm > l2 for lambda[50]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:20,]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:20,]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:9,]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:10,]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:12,]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:11,]
# Validation approach to estimate test error
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2) # another typical approach to sample
test=(-train)
y.test=y[test]
# fit a ridge regression model on the training set, and evaluate its MSE on the test set, using lambda = 4.
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,]) # Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
mean((ridge.pred-y.test)^2) # test MSE
mean((mean(y[train ])-y.test)^2) # test MSE, if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations.
# Validation approach to estimate test error
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2) # another typical approach to sample
test=(-train)
y.test=y[test]
# fit a ridge regression model on the training set, and evaluate its MSE on the test set, using lambda = 4.
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,]) # Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
mean((ridge.pred-y.test)^2) # test MSE
mean((mean(y[train ])-y.test)^2) # test MSE, if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations.
library(glmnet)
library(glmnet)
attach(tr_s)
attach(tr_s)
data_complete <- read.csv("dataset/data_complete.csv", header=TRUE)
head(data_complete)
names(data_complete)
install.packages('caTools')
library(caTools)
my_data <- data_complete[,c(3,7,8,9,10,11,12,13,14,15,16)]
split = sample.split(my_data$co2_emission, SplitRatio = 0.8)
tr_s = subset(my_data, split == TRUE)
t_s = subset(my_data, split == FALSE)
fit.linear <- (co2_emission ~ euro_standard + fuel_cost_6000_miles + fuel_type + engine_capacity + year + transmission_type
+ noise_level + combined_metric + urban_metric + extra_urban_metric)
x = model.matrix(fit.linear, tr_s)[,-1] #[-1] means no intercept
y = tr_s$co2_emission
grid=10^seq(10,-2,length=100) # Lambda values grid (from 10^10 to 10^-2)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
#Associated with each value of lambda is a vector of ridge regression
#coefficients, stored in a matrix that can be accessed by coef(),
#in this case 12x100, 11+intercept for each lambda value:
dim(coef(ridge.mod))
# We expect the coefficient estimates to be much smaller, in terms of l2 norm,
#when a large value of lambda is used, as compared to when a small value is used.
ridge.mod$lambda[50] # grid[50] = 11497.57
coef(ridge.mod)[,50] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # l2 norm = 0.33 (DISTANZA DA 0)
#60 ESIMO VALORE DI LAMBDA
ridge.mod$lambda[60] # lambda = 705.48
# We expect the coefficient estimates to be much smaller, in terms of l2 norm,
#when a large value of lambda is used, as compared to when a small value is used.
ridge.mod$lambda[50] # grid[50] = 11497.57
coef(ridge.mod)[,50] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # l2 norm = 0.33 (DISTANZA DA 0)
#60 ESIMO VALORE DI LAMBDA
ridge.mod$lambda[60] # lambda = 705.48
coef(ridge.mod)[,60] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,60]^2)) # l2 norm > l2 for lambda[50]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:11,]
# Validation approach to estimate test error
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2) # another typical approach to sample
test=(-train)
y.test=y[test]
# fit a ridge regression model on the training set, and evaluate its MSE on the test set, using lambda = 4.
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,]) # Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
mean((ridge.pred-y.test)^2) # test MSE
mean((mean(y[train ])-y.test)^2) # test MSE, if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations.
ridge.pred=predict(ridge.mod,s=10,newx=x[test,]) # Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
mean((ridge.pred-y.test)^2) # test MSE = 57
mean((mean(y[train ])-y.test)^2) # test MSE, if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations.
ridge.pred=predict(ridge.mod,s=2,newx=x[test,]) # Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
mean((ridge.pred-y.test)^2) # test MSE = 57
mean((mean(y[train ])-y.test)^2) # test MSE, if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations.
# fit a ridge regression model on the training set, and evaluate its MSE on the test set, using lambda = 4.
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=2,newx=x[test,]) # Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
mean((ridge.pred-y.test)^2) # test MSE = 57
mean((mean(y[train ])-y.test)^2) # test MSE, if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations.
# fit a ridge regression model on the training set, and evaluate its MSE on the test set, using lambda = 4.
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=10^10,newx=x[test,]) # Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
mean((ridge.pred-y.test)^2) # test MSE = 57
mean((mean(y[train ])-y.test)^2) # test MSE, if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations.
# Least squares is simply ridge regression with lambda=0;
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train]) # corrected according to errata (glmnet pack updated)
# PREDICTION WITH LAMBDA=, CORRESPONDING TO OLS
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train]) # corrected according to errata (glmnet pack updated)
mean((ridge.pred-y.test)^2)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
lm(y ~ x, subset=train)
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min;
bestlam;
log(bestlam) # the best lambda (212 on the text)
log(bestlam) # 1,72
cv.out$lambda.1se
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
log(bestlam) # 1,72
s
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min;
bestlam; #5,59
log(bestlam) # 1,72
cv.out$lambda.1se
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
bestlam=cv.out$lambda.min;
bestlam; #5,59
log(bestlam) # 1,72
cv.out$lambda.1se
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2)
# PREDICTION WITH LAMBDA= 0, CORRESPONDING TO LEAST SQUARE
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train]) # corrected according to errata (glmnet pack updated)
mean((ridge.pred-y.test)^2) #51.47307
# fit a ridge regression model on the training set, and evaluate its MSE on the test set, using lambda = 4.
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,]) # Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
mean((ridge.pred-y.test)^2) # test MSE = 57
mean((ridge.pred-y.test)^2) # test MSE = 87,90
mean((mean(y[train ])-y.test)^2) # test MSE, if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations.
# PREDICTION WITH LAMBDA= 10^10 , CORRESPONDING TO MODEL WITH ONLY INTERCEPT
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2) #  3229.959
# PREDICTION WITH LAMBDA= 0, CORRESPONDING TO LEAST SQUARE
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train]) # corrected according to errata (glmnet pack updated)
mean((ridge.pred-y.test)^2) #51.47307
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min;
bestlam; #5.598393
log(bestlam) # 1.72248
cv.out$lambda.1se
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2)
bestlam=cv.out$lambda.min;
bestlam; #5.598393
log(bestlam) # 1.72248
cv.out$lambda.1se
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:11,]
# As expected, none of the coefficients are zero
# ridge regression does not perform variable selection!
dev.new()
plot(out,label = T, xvar = "lambda")
plot(out,label = T, xvar = "lambda")
plot(out,label = T, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(X), cex = .5)
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .5)
plot.new()
plot(out,label = T, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .5)
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min;
bestlam; #5.598393
log(bestlam) # 1.72248
cv.out$lambda.1se
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2) #88.87  #IMPROVEMENT RESPECT LAMBDA=4
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:11,]
plot.new()
plot(out,label = T, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .5)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
dev.new()
plot(lasso.mod,label = T)
dev.new()
plot(lasso.mod,label = T, xvar = "lambda")
plot(lasso.mod,label = T, xvar = "lambda")
d
plot(lasso.mod,label = T, xvar = "lambda")
plot.new()
plot(lasso.mod,label = T, xvar = "lambda")
plot.new()
plot(lasso.mod,label = T, xvar = "lambda")
plot(lasso.mod,label = T, xvar = "lambda")
# perform cross-validation
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min; print(bestlam);print(log(bestlam))
print(cv.out$lambda.1se)
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2) # slighly larger than ridge
print(log(bestlam))
print(bestlam)
print(cv.out$lambda.1se)
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2) # slighly larger than ridge
# However, the lasso has a substantial advantage:
# 8 of the 19 coefficient estimates are exactly zero (12 on the text).
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:11,]
lasso.coef
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:11,]
lasso.coef
lasso.coef[lasso.coef!=0]
cat("Number of coefficients equal to 0:",sum(lasso.coef==0),"\n")
# compare with OLS when only selected predictors are included.
#fit.lm=lm(Salary~Hits+Walks+CRuns+CRBI+League+Division+PutOuts, data=Hitters) # 12 coeffs = 0 on the textbook
fit.lm=lm(Salary~AtBat+Hits+Walks+Years+CHmRun+CRuns+CRBI + League + Division+PutOuts+Errors, data=Hitters)
# compare with OLS when only selected predictors are included.
#fit.lm=lm(Salary~Hits+Walks+CRuns+CRBI+League+Division+PutOuts, data=Hitters) # 12 coeffs = 0 on the textbook
fit.lm=lm(fit.linear, data=tr_s)
coef(fit.lm)
fit.lm=lm(Salary~Hits+Walks+CRun data=tr_s)
fit.lm=lm(co2_emission~ extra_urban_metric, data=tr_s)
coef(fit.lm)
lasso.coef=predict(out,type="coefficients",s=0)[1:11,]
lasso.coef
fit.lm=lm(fit.linear <- (co2_emission ~ euro_standard + fuel_cost_6000_miles + fuel_type + engine_capacity + year + transmission_type
+ noise_level + combined_metric + urban_metric ), data=tr_s)
coef(fit.lm)
lasso.coef=predict(out,type="coefficients",s=0)[1:11,]
mean((lasso.pred-y.test)^2) # slighly larger than ridge #56,15
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
dev.new()
plot(lasso.mod,label = T)
dev.new()
plot.new()
plot(lasso.mod,label = T, xvar = "lambda")
# perform cross-validation
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min
print(bestlam) #0.10
print(log(bestlam)) #-2,20
print(cv.out$lambda.1se)
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2) # slighly larger than ridge #56,15
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min;
bestlam; #5.598393
log(bestlam) # 1.72248
cv.out$lambda.1se
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2) #88.87  #IMPROVEMENT OF MSE RESPECT LAMBDA=4
fit.lm=lm(fit.linear <- (co2_emission ~ euro_standard + fuel_cost_6000_miles + fuel_type + engine_capacity + year + transmission_type
+ noise_level + combined_metric + urban_metric ), data=tr_s)
lm.pred=predict(fit.lm,newx=x[test,])
lm.pred=predict(fit.lm,newx=x[test,])
mean((llm.pred-y.test)^2)
mean((lm.pred-y.test)^2)
fit.lm=lm(fit.linear <- (co2_emission ~ euro_standard + fuel_cost_6000_miles + fuel_type + engine_capacity + year + transmission_type
+ noise_level + combined_metric + urban_metric + extra_urban_metric), data=tr_s)
lm.pred=predict(fit.lm,newx=x[test,])
mean((lm.pred-y.test)^2)
fit.lm=lm(fit.linear <- (co2_emission ~ euro_standard + fuel_cost_6000_miles + fuel_type + engine_capacity + year + transmission_type
+ noise_level + combined_metric + urban_metric + extra_urban_metric), data=tr_s)
y_pred = predict(fit.lm, newdata = t_s, interval = 'confidence')
lm.pred=predict(fit.lm,newx=x[test,])
mean((lm.pred-y.test)^2)
coef(fit.lm)
summary(fit.lm)
y_pred = predict(fit.lm, newdata = t_s, interval = 'confidence')
lm.pred=predict(fit.lm,newx=x[test,])
mean((lm.pred-y.test)^2)
summary(fit.lm)
y_pred = predict(fit.lm, newdata = t_s, interval = 'confidence')
lm.pred=predict(fit.lm,newx=x[test,])
mean((lm.pred-y.t_s)^2)
lm.pred=predict(fit.lm,newx=t_s)
mean((lm.pred-y.test)^2)
y.test=y[t_s]
y.test=t_s
_
lm.pred=predict(fit.lm,newx=t_s)
mean((lm.pred-y.test)^2)
mean((lm.pred-y.test)^2)
lm.pred=predict(fit.lm,newx=x[test,])
mean((lm.pred-y.test)^2)
# PREDICTION WITH LAMBDA= 0, CORRESPONDING TO LEAST SQUARE
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train]) # corrected according to errata (glmnet pack updated)
mean((ridge.pred-y.test)^2) #51.47307
train=sample(1:nrow(x), nrow(x)/2) # another typical approach to sample
test=(-train)
y.test=y[test]
# PREDICTION WITH LAMBDA= 0, CORRESPONDING TO LEAST SQUARE
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train]) # corrected according to errata (glmnet pack updated)
mean((ridge.pred-y.test)^2) #51.47307
sqrt(sum((fit.linear$residuals)^2)/36339)
sqrt(sum((fit.linear$residuals)^2)/36339)
residui<-fit.linear$residuals
# PREDICTION WITH LAMBDA= 10^10 , CORRESPONDING TO MODEL WITH ONLY INTERCEPT
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2) #  3229.959
sqrt(sum((fit.linear$residui)^2)/36339)
mean((ridge.pred-y.test)^2) #  3229.959
sqrt(sum((ridge.mod$residui)^2)/36339)
library(glmnet)
attach(tr_s)
data_complete <- read.csv("dataset/data_complete.csv", header=TRUE)
head(data_complete)
names(data_complete)
# PREPREOCCESSING FOR ENCODING CATEGORICAL DATA
# data_complete$Colonna = factor(data_complete$Colonna, levels = c('', '', ''),
#                               labels = c(1,2,3))
install.packages('caTools')
library(caTools)
my_data <- data_complete[,c(3,7,8,9,10,11,12,13,14,15,16)]
split = sample.split(my_data$co2_emission, SplitRatio = 0.8)
tr_s = subset(my_data, split == TRUE)
t_s = subset(my_data, split == FALSE)
install.packages("caTools")
library(glmnet)
attach(tr_s)
fit.linear <- (co2_emission ~ euro_standard + fuel_cost_6000_miles + fuel_type + engine_capacity + year + transmission_type
+ noise_level + combined_metric + urban_metric + extra_urban_metric)
x = model.matrix(fit.linear, tr_s)[,-1] #[-1] means no intercept
y = tr_s$co2_emission
grid=10^seq(10,-2,length=100) # Lambda values grid (from 10^10 to 10^-2)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
#Associated with each value of lambda is a vector of ridge regression
#coefficients, stored in a matrix that can be accessed by coef(),
#in this case 12x100, 11+intercept for each lambda value:
dim(coef(ridge.mod))
# We expect the coefficient estimates to be much smaller, in terms of l2 norm,
#when a large value of lambda is used, as compared to when a small value is used.
ridge.mod$lambda[50] # grid[50] = 11497.57
coef(ridge.mod)[,50] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # l2 norm = 0.33 (DISTANZA DA 0)
#60 ESIMO VALORE DI LAMBDA
ridge.mod$lambda[60] # lambda = 705.48
coef(ridge.mod)[,60] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,60]^2)) # l2 norm = 3.82> l2 for lambda[50]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:11,]
# Validation approach to estimate test error
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2) # another typical approach to sample
test=(-train)
y.test=y[test]
# fit a ridge regression model on the training set, and evaluate its MSE on the test set, using lambda = 4.
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,]) # Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
mean((ridge.pred-y.test)^2) # test MSE = 87,90
mean((ridge.pred-y.test)^2) # test MSE = 58,59
mean((mean(y[train ])-y.test)^2) # test MSE, if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations.
# PREDICTION WITH LAMBDA= 10^10 , CORRESPONDING TO MODEL WITH ONLY INTERCEPT
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2) #  3229.959
mean((ridge.pred-y.test)^2) #  3206.242
# PREDICTION WITH LAMBDA= 0, CORRESPONDING TO LEAST SQUARE
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train]) # corrected according to errata (glmnet pack updated)
mean((ridge.pred-y.test)^2) #51.47307
mean((ridge.pred-y.test)^2) #69,41
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min;
bestlam; #5.598393
log(bestlam) # 1.72248
log(bestlam) # 1.726111
cv.out$lambda.1se
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2) #88.87  #IMPROVEMENT OF MSE RESPECT LAMBDA=4
mean((ridge.pred-y.test)^2) #64,23  #IMPROVEMENT OF MSE RESPECT LAMBDA=4
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:11,]
# As expected, none of the coefficients are zero
# ridge regression does not perform variable selection!
dev.new()
plot.new()
plot(out,label = T, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .5)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
dev.new()
plot(lasso.mod,label = T)
dev.new()
plot.new()
plot(lasso.mod,label = T, xvar = "lambda")
# perform cross-validation
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min
print(bestlam) #0.10
print(bestlam) #0.01563454
print(log(bestlam)) #-2,20
x = model.matrix(fit.linear, data_complete)[,-1] #[-1] means no intercept
y = data_complete$co2_emission
grid=10^seq(10,-2,length=100) # Lambda values grid (from 10^10 to 10^-2)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
#Associated with each value of lambda is a vector of ridge regression
#coefficients, stored in a matrix that can be accessed by coef(),
#in this case 12x100, 11+intercept for each lambda value:
dim(coef(ridge.mod))
# We expect the coefficient estimates to be much smaller, in terms of l2 norm,
#when a large value of lambda is used, as compared to when a small value is used.
ridge.mod$lambda[50] # grid[50] = 11497.57
coef(ridge.mod)[,50] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,50]^2)) # l2 norm = 0.33 (DISTANZA DA 0)
#60 ESIMO VALORE DI LAMBDA
ridge.mod$lambda[60] # lambda = 705.48
coef(ridge.mod)[,60] # corresponding coefficients
sqrt(sum(coef(ridge.mod)[-1,60]^2)) # l2 norm = 3.82> l2 for lambda[50]
# obtain the ridge regression coefficients for a new lambda, say 50:
predict(ridge.mod,s=50,type="coefficients")[1:11,]
# Validation approach to estimate test error
set.seed(1)
train=sample(1:nrow(x), nrow(x)/2) # another typical approach to sample
test=(-train)
y.test=y[test]
# fit a ridge regression model on the training set, and evaluate its MSE on the test set, using lambda = 4.
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,]) # Note the use of the predict() function again. This time we get predictions for a test set, by replacing type="coefficients" with the newx argument.
mean((ridge.pred-y.test)^2) # test MSE = 58,59
mean((mean(y[train ])-y.test)^2) # test MSE, if we had instead simply fit a model with just an intercept, we would have predicted each test observation using the mean of the training observations.
# PREDICTION WITH LAMBDA= 10^10 , CORRESPONDING TO MODEL WITH ONLY INTERCEPT
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2) #  3206.242
# PREDICTION WITH LAMBDA= 0, CORRESPONDING TO LEAST SQUARE
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train]) # corrected according to errata (glmnet pack updated)
mean((ridge.pred-y.test)^2) #69,41
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min;
bestlam; #5.618761
log(bestlam) # 1.726111
cv.out$lambda.1se
ridge.pred=predict(ridge.mod,s=bestlam ,newx=x[test,])
mean((ridge.pred-y.test)^2) #64,23  #IMPROVEMENT OF MSE RESPECT LAMBDA=4
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:11,]
# As expected, none of the coefficients are zero
# ridge regression does not perform variable selection!
dev.new()
plot.new()
plot(out,label = T, xvar = "lambda")
legend("bottomright", lwd = 1, col = 1:6, legend = colnames(x), cex = .5)
lasso.mod = glmnet(x[train,], y[train], alpha=1, lambda=grid)
dev.new()
plot(lasso.mod,label = T)
dev.new()
plot.new()
plot(lasso.mod,label = T, xvar = "lambda")
# perform cross-validation
set.seed (1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
dev.new()
plot(cv.out)
bestlam=cv.out$lambda.min
print(bestlam) #0.01563454
print(log(bestlam)) #-4.158273
print(cv.out$lambda.1se)
lasso.pred=predict(lasso.mod,s=bestlam ,newx=x[test,])
mean((lasso.pred-y.test)^2)  #56,15
lasso.pred=predict(lasso.mod,s=0,newx=x[test,],exact=T,x=x[train,],y=y[train])
mean((lasso.pred-y.test)^2)
# However, the lasso has a substantial advantage:
# 1 of the 11 coefficient estimates is exactly zero (extra_urban_metric) .
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:11,]
lasso.coef
lasso.coef[lasso.coef!=0]
cat("Number of coefficients equal to 0:",sum(lasso.coef==0),"\n")
